{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Define and Solve an ML Problem of Your Choosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab assignment, you will follow the machine learning life cycle and implement a model to solve a machine learning problem of your choosing. You will select a data set and choose a predictive problem that the data set supports.  You will then inspect the data with your problem in mind and begin to formulate a  project plan. You will then implement the machine learning project plan. \n",
    "\n",
    "You will complete the following tasks:\n",
    "\n",
    "1. Build Your DataFrame\n",
    "2. Define Your ML Problem\n",
    "3. Perform exploratory data analysis to understand your data.\n",
    "4. Define Your Project Plan\n",
    "5. Implement Your Project Plan:\n",
    "    * Prepare your data for your model.\n",
    "    * Fit your model to the training data and evaluate your model.\n",
    "    * Improve your model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Build Your DataFrame\n",
    "\n",
    "You will have the option to choose one of four data sets that you have worked with in this program:\n",
    "\n",
    "* The \"census\" data set that contains Census information from 1994: `censusData.csv`\n",
    "* Airbnb NYC \"listings\" data set: `airbnbListingsData.csv`\n",
    "* World Happiness Report (WHR) data set: `WHR2018Chapter2OnlineData.csv`\n",
    "* Book Review data set: `bookReviewsData.csv`\n",
    "\n",
    "Note that these are variations of the data sets that you have worked with in this program. For example, some do not include some of the preprocessing necessary for specific models. \n",
    "\n",
    "#### Load a Data Set and Save it as a Pandas DataFrame\n",
    "\n",
    "The code cell below contains filenames (path + filename) for each of the four data sets available to you.\n",
    "\n",
    "<b>Task:</b> In the code cell below, use the same method you have been using to load the data using `pd.read_csv()` and save it to DataFrame `df`. \n",
    "\n",
    "You can load each file as a new DataFrame to inspect the data before choosing your data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Positive Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This was perhaps the best of Johannes Steinhof...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This very fascinating book is a story written ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The four tales in this collection are beautifu...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The book contained more profanity than I expec...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We have now entered a second time of deep conc...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Positive Review\n",
       "0  This was perhaps the best of Johannes Steinhof...             True\n",
       "1  This very fascinating book is a story written ...             True\n",
       "2  The four tales in this collection are beautifu...             True\n",
       "3  The book contained more profanity than I expec...            False\n",
       "4  We have now entered a second time of deep conc...             True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File names of the four data sets\n",
    "adultDataSet_filename = os.path.join(os.getcwd(), \"data\", \"censusData.csv\")\n",
    "airbnbDataSet_filename = os.path.join(os.getcwd(), \"data\", \"airbnbListingsData.csv\")\n",
    "WHRDataSet_filename = os.path.join(os.getcwd(), \"data\", \"WHR2018Chapter2OnlineData.csv\")\n",
    "bookReviewDataSet_filename = os.path.join(os.getcwd(), \"data\", \"bookReviewsData.csv\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(bookReviewDataSet_filename) # YOUR CODE HERE\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Define Your ML Problem\n",
    "\n",
    "Next you will formulate your ML Problem. In the markdown cell below, answer the following questions:\n",
    "\n",
    "1. List the data set you have chosen.\n",
    "2. What will you be predicting? What is the label?\n",
    "3. Is this a supervised or unsupervised learning problem? Is this a clustering, classification or regression problem? Is it a binary classificaiton or multi-class classifiction problem?\n",
    "4. What are your features? (note: this list may change after your explore your data)\n",
    "5. Explain why this is an important problem. In other words, how would a company create value with a model that predicts this label?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Double click this Markdown cell to make it editable, and record your answers here.>\n",
    "1. I've chosen the book reviews data set (bookReviewsData.csv) for my project.\n",
    "2. Given that the data set only has two columns, \"Review\" and \"Positive Review\", I will be predicting the value of \"Positive Review\", represented by a boolean based on the data in the \"Review\" column. Thus, the label is \"Positive Review\" with possible values \"True\" and \"False.\"\n",
    "3. This is a supervised learning problem because the training data is labeled. This is a classifcation problem because we are trying to predict whether a review instance represents a positive or negative review.\n",
    "4. The only feature column is the \"Review\" column.\n",
    "5. This is an important problem because a model that is able to determine whether a given text review is postiive or negative can be used by a book review website to more accurately and efficiently determine which books are more well-liked than other books. Manually determining if a review is positive or negative is time consuming and categorization can vary between different people. Thus, using an ML model to accomplish this task will likely result in more consistent and faster results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Understand Your Data\n",
    "\n",
    "The next step is to perform exploratory data analysis. Inspect and analyze your data set with your machine learning problem in mind. Consider the following as you inspect your data:\n",
    "\n",
    "1. What data preparation techniques would you like to use? These data preparation techniques may include:\n",
    "\n",
    "    * addressing missingness, such as replacing missing values with means\n",
    "    * finding and replacing outliers\n",
    "    * renaming features and labels\n",
    "    * finding and replacing outliers\n",
    "    * performing feature engineering techniques such as one-hot encoding on categorical features\n",
    "    * selecting appropriate features and removing irrelevant features\n",
    "    * performing specific data cleaning and preprocessing techniques for an NLP problem\n",
    "    * addressing class imbalance in your data sample to promote fair AI\n",
    "    \n",
    "\n",
    "2. What machine learning model (or models) you would like to use that is suitable for your predictive problem and data?\n",
    "    * Are there other data preparation techniques that you will need to apply to build a balanced modeling data set for your problem and model? For example, will you need to scale your data?\n",
    " \n",
    " \n",
    "3. How will you evaluate and improve the model's performance?\n",
    "    * Are there specific evaluation metrics and methods that are appropriate for your model?\n",
    "    \n",
    "\n",
    "Think of the different techniques you have used to inspect and analyze your data in this course. These include using Pandas to apply data filters, using the Pandas `describe()` method to get insight into key statistics for each column, using the Pandas `dtypes` property to inspect the data type of each column, and using Matplotlib and Seaborn to detect outliers and visualize relationships between features and labels. If you are working on a classification problem, use techniques you have learned to determine if there is class imbalance.\n",
    "\n",
    "<b>Task</b>: Use the techniques you have learned in this course to inspect and analyze your data. You can import additional packages that you have used in this course that you will need to perform this task.\n",
    "\n",
    "<b>Note</b>: You can add code cells if needed by going to the <b>Insert</b> menu and clicking on <b>Insert Cell Below</b> in the drop-drown menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1973, 2)\n",
      "Index(['Review', 'Positive Review'], dtype='object')\n",
      "                                              Review  Positive Review\n",
      "0  This was perhaps the best of Johannes Steinhof...             True\n",
      "1  This very fascinating book is a story written ...             True\n",
      "2  The four tales in this collection are beautifu...             True\n",
      "3  The book contained more profanity than I expec...            False\n",
      "4  We have now entered a second time of deep conc...             True\n",
      "5  I don't know why it won the National Book Awar...            False\n",
      "6  The daughter of a prominent Boston doctor is d...            False\n",
      "7  I was very disapointed in the book.Basicly the...            False\n",
      "8  I think in retrospect I wasted my time on this...            False\n",
      "9  I have a hard time understanding what it is th...            False\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# inspect the data at a glance\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(df.head(10))\n",
    "print(pd.isnull(df[\"Review\"]).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: there is not a lot of data preprocessing/cleanup that can be done prior to vectorizing the data in the \"Review\" column (there are no null columns, there is only one feature). This work will be done in Part 5 as a part of the model implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Define Your Project Plan\n",
    "\n",
    "Now that you understand your data, in the markdown cell below, define your plan to implement the remaining phases of the machine learning life cycle (data preparation, modeling, evaluation) to solve your ML problem. Answer the following questions:\n",
    "\n",
    "* Do you have a new feature list? If so, what are the features that you chose to keep and remove after inspecting the data?Â \n",
    "* Explain different data preparation techniques that you will use to prepare your data for modeling.\n",
    "* What is your model (or models)?\n",
    "* Describe your plan to train your model, analyze its performance and then improve the model. That is, describe your model building, validation and selection plan to produce a model that generalizes well to new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Double click this Markdown cell to make it editable, and record your answers here.>\n",
    "\n",
    "<b>Preparing Data</b>\n",
    "The data is split into the label \"Positive Review\" and the features \"Review\", then train_test_split is applied to separate the data further into a training and testing set. After applying the tfidf_vectorizer, data from the \"Review\" column is split into individual words/n-grams, which are then assigned weights based on frequency and importance. This information is used to create a feature matrix where each document is represented by a vector of these weights. The inclusion/exclusion of certain features is based off of the tfidf_vectorizer hyperparameters min_df and ngram_range, which specify the minimum frequency and the size of terms respectively. \n",
    "\n",
    "<b>Model Selection</b>\n",
    "I've chosen to use a neural network implemented via keras's Sequential API. The neural network will consist of an input layer that is the same size as the TF-IDF vector, several hidden layers with ReLU activation functions with a logarithmically decreasing number of nodes, and an output layer with a sigmoid activation function (the output layer is sigmoid because the problem is binary classifcation).\n",
    "\n",
    "<b>Model Training and Improvement</b>\n",
    "After transforming the data with the TF-IDF vectorizer and building the neural network as specified above, I will methodically optimize hyperparameters min_df, ngram_range, hidden_layer_units, and learning_rate based on the metric accuracy. I opt to optimize one hyperparameter at a time instead of performing an exhaustive search (i.e. gridsearch) of the best hyperparameter combination because training numerous separate neural networks takes a lot of time, especially when implementing changes like increasing ngram_range or increasing complexity of hidden layers. I will iterate through possible values for vectorizer parameters min_df and ngram_range first before the model parameters, because the former deal directly with the feature space and will likely be more impactful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Implement Your Project Plan\n",
    "\n",
    "<b>Task:</b> In the code cell below, import additional packages that you have used in this course that you will need to implement your project plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\" # suppress info and warning messages\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras as keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b> Use the rest of this notebook to carry out your project plan. \n",
    "\n",
    "You will:\n",
    "\n",
    "1. Prepare your data for your model.\n",
    "2. Fit your model to the training data and evaluate your model.\n",
    "3. Improve your model's performance by performing model selection and/or feature selection techniques to find best model for your problem.\n",
    "\n",
    "Add code cells below and populate the notebook with commentary, code, analyses, results, and figures as you see fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, min_df=1, ngram_range=(1, 1)):\n",
    "    y = df[\"Positive Review\"]\n",
    "    X = df[\"Review\"]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words='english', min_df=min_df, ngram_range=ngram_range)\n",
    "    \n",
    "    tfidf_vectorizer.fit(X_train)\n",
    "    \n",
    "    X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "    \n",
    "    return X_train_tfidf, X_test_tfidf, y_train, y_test, len(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and Compile Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocabulary_size, learning_rate=0.1, hidden_layers_units=[64, 32, 16]):\n",
    "    nn_model = keras.Sequential()\n",
    "    nn_model.add(keras.layers.InputLayer(input_shape=(vocabulary_size,)))\n",
    "    \n",
    "    for units in hidden_layers_units:\n",
    "        nn_model.add(keras.layers.Dense(units=units, activation='relu'))\n",
    "    \n",
    "    nn_model.add(keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "    \n",
    "    sgd_optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    loss_fn = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    \n",
    "    nn_model.compile(optimizer=sgd_optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "    \n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgBarLoggerNEpochs(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, num_epochs: int, every_n: int = 50):\n",
    "        self.num_epochs = num_epochs\n",
    "        self.every_n = every_n\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.every_n == 0:\n",
    "            s = 'Epoch [{}/ {}]'.format(epoch + 1, self.num_epochs)\n",
    "            logs_s = ['{}: {:.4f}'.format(k.capitalize(), v)\n",
    "                      for k, v in logs.items()]\n",
    "            s_list = [s] + logs_s\n",
    "            print(', '.join(s_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(nn_model, X_train_tfidf, y_train, num_epochs=55):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    history = nn_model.fit(X_train_tfidf.toarray(), y_train, epochs=num_epochs, verbose=0, validation_split=0.2, \n",
    "                           callbacks=[ProgBarLoggerNEpochs(num_epochs, every_n=5)])\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print('Elapsed time: %.2fs' % (t1-t0))\n",
    "    \n",
    "    return nn_model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(nn_model, X_test_tfidf, y_test):\n",
    "    loss, accuracy = nn_model.evaluate(X_test_tfidf.toarray(), y_test)\n",
    "    print('Loss: ', str(loss), 'Accuracy: ', str(accuracy))\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "min_df_values = [1, 2, 4, 8, 12, 16]\n",
    "ngram_ranges = [(1, 1), (1, 2), (1, 3), (2, 3), (1, 4)]\n",
    "learning_rates = [0.01, 0.1]\n",
    "hidden_layers_units_list = [[64, 32, 16], [128, 64, 32]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 21:42:20.571553: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/ 55], Loss: 0.6905, Accuracy: 0.5376, Val_loss: 0.6892, Val_accuracy: 0.5338\n",
      "Epoch [10/ 55], Loss: 0.6714, Accuracy: 0.6965, Val_loss: 0.6780, Val_accuracy: 0.6318\n",
      "Epoch [15/ 55], Loss: 0.5676, Accuracy: 0.7675, Val_loss: 0.6825, Val_accuracy: 0.4932\n",
      "Epoch [20/ 55], Loss: 0.4688, Accuracy: 0.7684, Val_loss: 0.5405, Val_accuracy: 0.7230\n",
      "Epoch [25/ 55], Loss: 0.3284, Accuracy: 0.8478, Val_loss: 0.7888, Val_accuracy: 0.5912\n",
      "Epoch [30/ 55], Loss: 0.7081, Accuracy: 0.5080, Val_loss: 0.6822, Val_accuracy: 0.5946\n",
      "Epoch [35/ 55], Loss: 0.4723, Accuracy: 0.7354, Val_loss: 0.7061, Val_accuracy: 0.5574\n",
      "Epoch [40/ 55], Loss: 0.3882, Accuracy: 0.8166, Val_loss: 0.8302, Val_accuracy: 0.6014\n",
      "Epoch [45/ 55], Loss: 0.0253, Accuracy: 1.0000, Val_loss: 0.3878, Val_accuracy: 0.8378\n",
      "Epoch [50/ 55], Loss: 0.0065, Accuracy: 1.0000, Val_loss: 0.4146, Val_accuracy: 0.8412\n",
      "Epoch [55/ 55], Loss: 0.0032, Accuracy: 1.0000, Val_loss: 0.4400, Val_accuracy: 0.8378\n",
      "Elapsed time: 5.06s\n",
      "16/16 [==============================] - 0s 903us/step - loss: 0.5804 - accuracy: 0.7976\n",
      "Loss:  0.5803675055503845 Accuracy:  0.7975708246231079\n",
      "Epoch [5/ 55], Loss: 0.6900, Accuracy: 0.5435, Val_loss: 0.6903, Val_accuracy: 0.5541\n",
      "Epoch [10/ 55], Loss: 0.6683, Accuracy: 0.7050, Val_loss: 0.6747, Val_accuracy: 0.7297\n",
      "Epoch [15/ 55], Loss: 0.5394, Accuracy: 0.7134, Val_loss: 0.5527, Val_accuracy: 0.7568\n",
      "Epoch [20/ 55], Loss: 0.4876, Accuracy: 0.7616, Val_loss: 0.4644, Val_accuracy: 0.7770\n",
      "Epoch [25/ 55], Loss: 0.2940, Accuracy: 0.8673, Val_loss: 0.6564, Val_accuracy: 0.6791\n",
      "Epoch [30/ 55], Loss: 0.0291, Accuracy: 0.9992, Val_loss: 0.4554, Val_accuracy: 0.7872\n",
      "Epoch [35/ 55], Loss: 0.0068, Accuracy: 1.0000, Val_loss: 0.5108, Val_accuracy: 0.8007\n",
      "Epoch [40/ 55], Loss: 0.0034, Accuracy: 1.0000, Val_loss: 0.5492, Val_accuracy: 0.8007\n",
      "Epoch [45/ 55], Loss: 0.0021, Accuracy: 1.0000, Val_loss: 0.5699, Val_accuracy: 0.7973\n",
      "Epoch [50/ 55], Loss: 0.0015, Accuracy: 1.0000, Val_loss: 0.5907, Val_accuracy: 0.8007\n",
      "Epoch [55/ 55], Loss: 0.0012, Accuracy: 1.0000, Val_loss: 0.6068, Val_accuracy: 0.8007\n",
      "Elapsed time: 3.56s\n",
      "16/16 [==============================] - 0s 737us/step - loss: 0.6225 - accuracy: 0.8178\n",
      "Loss:  0.6225284934043884 Accuracy:  0.8178137540817261\n",
      "Epoch [5/ 55], Loss: 0.6891, Accuracy: 0.5368, Val_loss: 0.6885, Val_accuracy: 0.5709\n",
      "Epoch [10/ 55], Loss: 0.6516, Accuracy: 0.7371, Val_loss: 0.6580, Val_accuracy: 0.5912\n",
      "Epoch [15/ 55], Loss: 0.5473, Accuracy: 0.7244, Val_loss: 0.6502, Val_accuracy: 0.6047\n",
      "Epoch [20/ 55], Loss: 0.3292, Accuracy: 0.8555, Val_loss: 0.4111, Val_accuracy: 0.8074\n",
      "Epoch [25/ 55], Loss: 0.1898, Accuracy: 0.9425, Val_loss: 0.3940, Val_accuracy: 0.8209\n",
      "Epoch [30/ 55], Loss: 0.0406, Accuracy: 0.9949, Val_loss: 0.4557, Val_accuracy: 0.8209\n",
      "Epoch [35/ 55], Loss: 0.0077, Accuracy: 1.0000, Val_loss: 0.4821, Val_accuracy: 0.8209\n",
      "Epoch [40/ 55], Loss: 0.0038, Accuracy: 1.0000, Val_loss: 0.5104, Val_accuracy: 0.8277\n",
      "Epoch [45/ 55], Loss: 0.0024, Accuracy: 1.0000, Val_loss: 0.5251, Val_accuracy: 0.8209\n",
      "Epoch [50/ 55], Loss: 0.0017, Accuracy: 1.0000, Val_loss: 0.5501, Val_accuracy: 0.8277\n",
      "Epoch [55/ 55], Loss: 0.0013, Accuracy: 1.0000, Val_loss: 0.5555, Val_accuracy: 0.8209\n",
      "Elapsed time: 2.84s\n",
      "16/16 [==============================] - 0s 649us/step - loss: 0.8240 - accuracy: 0.7814\n",
      "Loss:  0.8239578604698181 Accuracy:  0.7813765406608582\n",
      "Epoch [5/ 55], Loss: 0.6833, Accuracy: 0.5554, Val_loss: 0.6883, Val_accuracy: 0.5743\n",
      "Epoch [10/ 55], Loss: 0.6146, Accuracy: 0.7337, Val_loss: 0.6199, Val_accuracy: 0.7365\n",
      "Epoch [15/ 55], Loss: 0.4325, Accuracy: 0.7929, Val_loss: 0.8652, Val_accuracy: 0.5405\n",
      "Epoch [20/ 55], Loss: 0.3874, Accuracy: 0.8030, Val_loss: 0.4841, Val_accuracy: 0.7568\n",
      "Epoch [25/ 55], Loss: 0.2243, Accuracy: 0.9087, Val_loss: 1.2353, Val_accuracy: 0.5845\n",
      "Epoch [30/ 55], Loss: 0.0469, Accuracy: 0.9941, Val_loss: 0.6331, Val_accuracy: 0.7905\n",
      "Epoch [35/ 55], Loss: 0.0256, Accuracy: 0.9975, Val_loss: 0.6042, Val_accuracy: 0.8074\n",
      "Epoch [40/ 55], Loss: 0.0112, Accuracy: 0.9992, Val_loss: 0.7036, Val_accuracy: 0.8007\n",
      "Epoch [45/ 55], Loss: 0.0044, Accuracy: 1.0000, Val_loss: 0.7686, Val_accuracy: 0.8007\n",
      "Epoch [50/ 55], Loss: 0.0024, Accuracy: 1.0000, Val_loss: 0.8383, Val_accuracy: 0.7939\n",
      "Epoch [55/ 55], Loss: 0.0018, Accuracy: 1.0000, Val_loss: 0.8743, Val_accuracy: 0.7939\n",
      "Elapsed time: 2.50s\n",
      "16/16 [==============================] - 0s 558us/step - loss: 0.9310 - accuracy: 0.7672\n",
      "Loss:  0.9310351014137268 Accuracy:  0.7672064900398254\n",
      "Epoch [5/ 55], Loss: 0.6858, Accuracy: 0.5613, Val_loss: 0.6875, Val_accuracy: 0.5203\n",
      "Epoch [10/ 55], Loss: 0.6128, Accuracy: 0.7616, Val_loss: 0.6161, Val_accuracy: 0.7601\n",
      "Epoch [15/ 55], Loss: 0.4374, Accuracy: 0.7980, Val_loss: 0.5084, Val_accuracy: 0.7500\n",
      "Epoch [20/ 55], Loss: 0.3439, Accuracy: 0.8495, Val_loss: 0.4729, Val_accuracy: 0.7635\n",
      "Epoch [25/ 55], Loss: 0.2348, Accuracy: 0.9214, Val_loss: 0.5463, Val_accuracy: 0.7568\n",
      "Epoch [30/ 55], Loss: 0.2869, Accuracy: 0.9121, Val_loss: 0.6540, Val_accuracy: 0.6250\n",
      "Epoch [35/ 55], Loss: 0.1431, Accuracy: 0.9569, Val_loss: 0.6361, Val_accuracy: 0.7568\n",
      "Epoch [40/ 55], Loss: 0.0092, Accuracy: 1.0000, Val_loss: 0.7938, Val_accuracy: 0.7669\n",
      "Epoch [45/ 55], Loss: 0.0044, Accuracy: 1.0000, Val_loss: 0.8803, Val_accuracy: 0.7568\n",
      "Epoch [50/ 55], Loss: 0.0028, Accuracy: 1.0000, Val_loss: 0.9401, Val_accuracy: 0.7635\n",
      "Epoch [55/ 55], Loss: 0.0020, Accuracy: 1.0000, Val_loss: 0.9840, Val_accuracy: 0.7635\n",
      "Elapsed time: 2.41s\n",
      "16/16 [==============================] - 0s 587us/step - loss: 0.8315 - accuracy: 0.7996\n",
      "Loss:  0.8314835429191589 Accuracy:  0.7995951175689697\n",
      "Epoch [5/ 55], Loss: 0.6853, Accuracy: 0.5841, Val_loss: 0.6851, Val_accuracy: 0.5980\n",
      "Epoch [10/ 55], Loss: 0.6190, Accuracy: 0.7329, Val_loss: 0.6323, Val_accuracy: 0.6182\n",
      "Epoch [15/ 55], Loss: 0.4680, Accuracy: 0.7726, Val_loss: 0.6502, Val_accuracy: 0.6520\n",
      "Epoch [20/ 55], Loss: 0.3386, Accuracy: 0.8698, Val_loss: 0.4672, Val_accuracy: 0.7770\n",
      "Epoch [25/ 55], Loss: 0.1826, Accuracy: 0.9307, Val_loss: 0.7674, Val_accuracy: 0.7162\n",
      "Epoch [30/ 55], Loss: 0.0975, Accuracy: 0.9763, Val_loss: 0.6020, Val_accuracy: 0.7838\n",
      "Epoch [35/ 55], Loss: 0.0275, Accuracy: 0.9983, Val_loss: 0.7067, Val_accuracy: 0.7905\n",
      "Epoch [40/ 55], Loss: 0.0093, Accuracy: 1.0000, Val_loss: 0.8413, Val_accuracy: 0.7905\n",
      "Epoch [45/ 55], Loss: 0.0046, Accuracy: 1.0000, Val_loss: 0.9180, Val_accuracy: 0.7973\n",
      "Epoch [50/ 55], Loss: 0.0028, Accuracy: 1.0000, Val_loss: 0.9758, Val_accuracy: 0.7973\n",
      "Epoch [55/ 55], Loss: 0.0021, Accuracy: 1.0000, Val_loss: 1.0170, Val_accuracy: 0.7872\n",
      "Elapsed time: 2.31s\n",
      "16/16 [==============================] - 0s 568us/step - loss: 1.0576 - accuracy: 0.7591\n",
      "Loss:  1.0576115846633911 Accuracy:  0.7591093182563782\n"
     ]
    }
   ],
   "source": [
    "# find best min_df\n",
    "best_accuracy = 0\n",
    "best_min_df = min_df_values[0]\n",
    "for min_df in min_df_values:\n",
    "    X_train_tfidf, X_test_tfidf, y_train, y_test, vocabulary_size = prepare_data(df, min_df=min_df, ngram_range=(1, 1))\n",
    "    nn_model = build_model(vocabulary_size, learning_rate=0.1, hidden_layers_units=[64, 32, 16])\n",
    "    nn_model, _ = train_model(nn_model, X_train_tfidf, y_train, num_epochs=55)\n",
    "    _, accuracy = evaluate_model(nn_model, X_test_tfidf, y_test)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_min_df = min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/ 55], Loss: 0.6836, Accuracy: 0.5773, Val_loss: 0.6848, Val_accuracy: 0.6115\n",
      "Epoch [10/ 55], Loss: 0.5762, Accuracy: 0.7422, Val_loss: 0.6152, Val_accuracy: 0.6588\n",
      "Epoch [15/ 55], Loss: 0.5007, Accuracy: 0.7599, Val_loss: 0.6243, Val_accuracy: 0.6588\n",
      "Epoch [20/ 55], Loss: 0.2848, Accuracy: 0.8859, Val_loss: 0.4921, Val_accuracy: 0.7635\n",
      "Epoch [25/ 55], Loss: 0.0662, Accuracy: 0.9924, Val_loss: 0.4890, Val_accuracy: 0.7703\n",
      "Epoch [30/ 55], Loss: 0.0102, Accuracy: 1.0000, Val_loss: 0.5679, Val_accuracy: 0.7601\n",
      "Epoch [35/ 55], Loss: 0.0041, Accuracy: 1.0000, Val_loss: 0.6101, Val_accuracy: 0.7635\n",
      "Epoch [40/ 55], Loss: 0.0024, Accuracy: 1.0000, Val_loss: 0.6395, Val_accuracy: 0.7601\n",
      "Epoch [45/ 55], Loss: 0.0017, Accuracy: 1.0000, Val_loss: 0.6607, Val_accuracy: 0.7568\n",
      "Epoch [50/ 55], Loss: 0.0013, Accuracy: 1.0000, Val_loss: 0.6792, Val_accuracy: 0.7601\n",
      "Epoch [55/ 55], Loss: 0.0010, Accuracy: 1.0000, Val_loss: 0.6959, Val_accuracy: 0.7635\n",
      "Elapsed time: 3.55s\n",
      "16/16 [==============================] - 0s 728us/step - loss: 0.6629 - accuracy: 0.7915\n",
      "Loss:  0.6629130244255066 Accuracy:  0.7914980053901672\n",
      "Epoch [5/ 55], Loss: 0.6904, Accuracy: 0.5452, Val_loss: 0.6940, Val_accuracy: 0.4764\n",
      "Epoch [10/ 55], Loss: 0.6423, Accuracy: 0.7794, Val_loss: 0.6631, Val_accuracy: 0.6689\n",
      "Epoch [15/ 55], Loss: 0.5282, Accuracy: 0.7481, Val_loss: 0.7180, Val_accuracy: 0.5574\n",
      "Epoch [20/ 55], Loss: 0.3234, Accuracy: 0.8690, Val_loss: 0.4415, Val_accuracy: 0.8041\n",
      "Epoch [25/ 55], Loss: 0.3343, Accuracy: 0.9003, Val_loss: 0.5405, Val_accuracy: 0.7095\n",
      "Epoch [30/ 55], Loss: 0.0106, Accuracy: 1.0000, Val_loss: 0.4358, Val_accuracy: 0.8243\n",
      "Epoch [35/ 55], Loss: 0.0043, Accuracy: 1.0000, Val_loss: 0.4612, Val_accuracy: 0.8041\n",
      "Epoch [40/ 55], Loss: 0.0025, Accuracy: 1.0000, Val_loss: 0.4728, Val_accuracy: 0.8176\n",
      "Epoch [45/ 55], Loss: 0.0017, Accuracy: 1.0000, Val_loss: 0.4835, Val_accuracy: 0.8176\n",
      "Epoch [50/ 55], Loss: 0.0013, Accuracy: 1.0000, Val_loss: 0.4943, Val_accuracy: 0.8108\n",
      "Epoch [55/ 55], Loss: 0.0010, Accuracy: 1.0000, Val_loss: 0.5025, Val_accuracy: 0.8108\n",
      "Elapsed time: 4.44s\n",
      "16/16 [==============================] - 0s 902us/step - loss: 0.6821 - accuracy: 0.7834\n",
      "Loss:  0.6820620894432068 Accuracy:  0.78340083360672\n",
      "Epoch [5/ 55], Loss: 0.6898, Accuracy: 0.5520, Val_loss: 0.6894, Val_accuracy: 0.5338\n",
      "Epoch [10/ 55], Loss: 0.6592, Accuracy: 0.7041, Val_loss: 0.6816, Val_accuracy: 0.4764\n",
      "Epoch [15/ 55], Loss: 0.5736, Accuracy: 0.6872, Val_loss: 0.5870, Val_accuracy: 0.7432\n",
      "Epoch [20/ 55], Loss: 0.4106, Accuracy: 0.8166, Val_loss: 0.4920, Val_accuracy: 0.7838\n",
      "Epoch [25/ 55], Loss: 0.2860, Accuracy: 0.8867, Val_loss: 0.4979, Val_accuracy: 0.7601\n",
      "Epoch [30/ 55], Loss: 0.0167, Accuracy: 1.0000, Val_loss: 0.5461, Val_accuracy: 0.7770\n",
      "Epoch [35/ 55], Loss: 0.0055, Accuracy: 1.0000, Val_loss: 0.6023, Val_accuracy: 0.7770\n",
      "Epoch [40/ 55], Loss: 0.0029, Accuracy: 1.0000, Val_loss: 0.6445, Val_accuracy: 0.7770\n",
      "Epoch [45/ 55], Loss: 0.0019, Accuracy: 1.0000, Val_loss: 0.6652, Val_accuracy: 0.7838\n",
      "Epoch [50/ 55], Loss: 0.0014, Accuracy: 1.0000, Val_loss: 0.6847, Val_accuracy: 0.7838\n",
      "Epoch [55/ 55], Loss: 0.0010, Accuracy: 1.0000, Val_loss: 0.7019, Val_accuracy: 0.7838\n",
      "Elapsed time: 6.01s\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5605 - accuracy: 0.8117\n",
      "Loss:  0.5604909658432007 Accuracy:  0.8117408752441406\n",
      "Epoch [5/ 55], Loss: 0.6924, Accuracy: 0.5173, Val_loss: 0.6925, Val_accuracy: 0.5541\n",
      "Epoch [10/ 55], Loss: 0.6852, Accuracy: 0.5723, Val_loss: 0.6896, Val_accuracy: 0.6284\n",
      "Epoch [15/ 55], Loss: 0.6492, Accuracy: 0.6999, Val_loss: 0.6761, Val_accuracy: 0.5608\n",
      "Epoch [20/ 55], Loss: 0.5315, Accuracy: 0.7413, Val_loss: 0.6004, Val_accuracy: 0.6959\n",
      "Epoch [25/ 55], Loss: 0.3938, Accuracy: 0.8208, Val_loss: 1.0139, Val_accuracy: 0.5101\n",
      "Epoch [30/ 55], Loss: 0.2809, Accuracy: 0.8859, Val_loss: 0.5680, Val_accuracy: 0.6926\n",
      "Epoch [35/ 55], Loss: 0.1898, Accuracy: 0.9484, Val_loss: 0.5776, Val_accuracy: 0.7162\n",
      "Epoch [40/ 55], Loss: 0.0523, Accuracy: 0.9941, Val_loss: 0.6172, Val_accuracy: 0.6892\n",
      "Epoch [45/ 55], Loss: 0.0207, Accuracy: 0.9932, Val_loss: 0.7805, Val_accuracy: 0.7162\n",
      "Epoch [50/ 55], Loss: 0.0198, Accuracy: 0.9958, Val_loss: 0.7328, Val_accuracy: 0.6993\n",
      "Epoch [55/ 55], Loss: 0.0213, Accuracy: 0.9924, Val_loss: 0.9463, Val_accuracy: 0.6588\n",
      "Elapsed time: 4.04s\n",
      "16/16 [==============================] - 0s 815us/step - loss: 1.0299 - accuracy: 0.6761\n",
      "Loss:  1.0298796892166138 Accuracy:  0.6761133670806885\n",
      "Epoch [5/ 55], Loss: 0.6889, Accuracy: 0.5706, Val_loss: 0.6896, Val_accuracy: 0.5068\n",
      "Epoch [10/ 55], Loss: 0.6638, Accuracy: 0.7312, Val_loss: 0.6843, Val_accuracy: 0.4932\n",
      "Epoch [15/ 55], Loss: 0.6062, Accuracy: 0.6306, Val_loss: 0.5665, Val_accuracy: 0.6858\n",
      "Epoch [20/ 55], Loss: 0.4799, Accuracy: 0.7540, Val_loss: 1.0886, Val_accuracy: 0.5169\n",
      "Epoch [25/ 55], Loss: 0.1122, Accuracy: 0.9806, Val_loss: 0.4012, Val_accuracy: 0.8176\n",
      "Epoch [30/ 55], Loss: 0.0176, Accuracy: 1.0000, Val_loss: 0.3801, Val_accuracy: 0.8480\n",
      "Epoch [35/ 55], Loss: 0.0055, Accuracy: 1.0000, Val_loss: 0.4077, Val_accuracy: 0.8581\n",
      "Epoch [40/ 55], Loss: 0.0029, Accuracy: 1.0000, Val_loss: 0.4233, Val_accuracy: 0.8615\n",
      "Epoch [45/ 55], Loss: 0.0019, Accuracy: 1.0000, Val_loss: 0.4337, Val_accuracy: 0.8547\n",
      "Epoch [50/ 55], Loss: 0.0014, Accuracy: 1.0000, Val_loss: 0.4447, Val_accuracy: 0.8581\n",
      "Epoch [55/ 55], Loss: 0.0011, Accuracy: 1.0000, Val_loss: 0.4529, Val_accuracy: 0.8581\n",
      "Elapsed time: 7.44s\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.7027 - accuracy: 0.7632\n",
      "Loss:  0.7027353048324585 Accuracy:  0.7631579041481018\n"
     ]
    }
   ],
   "source": [
    "# find best ngram_range\n",
    "best_accuracy = 0\n",
    "best_ngram_range = ngram_ranges[0]\n",
    "for ngram_range in ngram_ranges:\n",
    "    X_train_tfidf, X_test_tfidf, y_train, y_test, vocabulary_size = prepare_data(df, min_df=best_min_df, ngram_range=ngram_range)\n",
    "    nn_model = build_model(vocabulary_size, learning_rate=0.1, hidden_layers_units=[64, 32, 16])\n",
    "    nn_model, _ = train_model(nn_model, X_train_tfidf, y_train, num_epochs=55)\n",
    "    _, accuracy = evaluate_model(nn_model, X_test_tfidf, y_test)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_ngram_range = ngram_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/ 55], Loss: 0.6901, Accuracy: 0.5385, Val_loss: 0.6908, Val_accuracy: 0.5135\n",
      "Epoch [10/ 55], Loss: 0.6662, Accuracy: 0.6948, Val_loss: 0.6757, Val_accuracy: 0.7365\n",
      "Epoch [15/ 55], Loss: 0.5493, Accuracy: 0.7329, Val_loss: 0.8755, Val_accuracy: 0.5135\n",
      "Epoch [20/ 55], Loss: 0.4012, Accuracy: 0.8166, Val_loss: 1.2279, Val_accuracy: 0.5000\n",
      "Epoch [25/ 55], Loss: 0.1064, Accuracy: 0.9865, Val_loss: 0.4176, Val_accuracy: 0.8007\n",
      "Epoch [30/ 55], Loss: 0.0137, Accuracy: 1.0000, Val_loss: 0.4339, Val_accuracy: 0.7905\n",
      "Epoch [35/ 55], Loss: 0.0049, Accuracy: 1.0000, Val_loss: 0.4610, Val_accuracy: 0.8007\n",
      "Epoch [40/ 55], Loss: 0.0027, Accuracy: 1.0000, Val_loss: 0.4800, Val_accuracy: 0.8007\n",
      "Epoch [45/ 55], Loss: 0.0018, Accuracy: 1.0000, Val_loss: 0.4975, Val_accuracy: 0.8041\n",
      "Epoch [50/ 55], Loss: 0.0013, Accuracy: 1.0000, Val_loss: 0.5068, Val_accuracy: 0.8041\n",
      "Epoch [55/ 55], Loss: 0.0010, Accuracy: 1.0000, Val_loss: 0.5167, Val_accuracy: 0.8041\n",
      "Elapsed time: 5.26s\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5983 - accuracy: 0.7955\n",
      "Loss:  0.5982879996299744 Accuracy:  0.7955465316772461\n",
      "Epoch [5/ 55], Loss: 0.6907, Accuracy: 0.5554, Val_loss: 0.6927, Val_accuracy: 0.4730\n",
      "Epoch [10/ 55], Loss: 0.6509, Accuracy: 0.7346, Val_loss: 0.6659, Val_accuracy: 0.6351\n",
      "Epoch [15/ 55], Loss: 0.5517, Accuracy: 0.6746, Val_loss: 0.5696, Val_accuracy: 0.7872\n",
      "Epoch [20/ 55], Loss: 0.4007, Accuracy: 0.8428, Val_loss: 0.5015, Val_accuracy: 0.7703\n",
      "Epoch [25/ 55], Loss: 0.3625, Accuracy: 0.8673, Val_loss: 0.4860, Val_accuracy: 0.7872\n",
      "Epoch [30/ 55], Loss: 0.0138, Accuracy: 1.0000, Val_loss: 0.5546, Val_accuracy: 0.7905\n",
      "Epoch [35/ 55], Loss: 0.0048, Accuracy: 1.0000, Val_loss: 0.6063, Val_accuracy: 0.7872\n",
      "Epoch [40/ 55], Loss: 0.0027, Accuracy: 1.0000, Val_loss: 0.6379, Val_accuracy: 0.7838\n",
      "Epoch [45/ 55], Loss: 0.0018, Accuracy: 1.0000, Val_loss: 0.6626, Val_accuracy: 0.7770\n",
      "Epoch [50/ 55], Loss: 0.0013, Accuracy: 1.0000, Val_loss: 0.6812, Val_accuracy: 0.7872\n",
      "Epoch [55/ 55], Loss: 0.0010, Accuracy: 1.0000, Val_loss: 0.6976, Val_accuracy: 0.7770\n",
      "Elapsed time: 7.91s\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5530 - accuracy: 0.8219\n",
      "Loss:  0.5530160665512085 Accuracy:  0.8218623399734497\n"
     ]
    }
   ],
   "source": [
    "# find best hidden_layers_units\n",
    "best_accuracy = 0\n",
    "best_hidden_layers_units = hidden_layers_units_list[0]\n",
    "for hidden_layers_units in hidden_layers_units_list:\n",
    "    X_train_tfidf, X_test_tfidf, y_train, y_test, vocabulary_size = prepare_data(df, min_df=best_min_df, ngram_range=best_ngram_range)\n",
    "    nn_model = build_model(vocabulary_size, learning_rate=0.1, hidden_layers_units=hidden_layers_units)\n",
    "    nn_model, _ = train_model(nn_model, X_train_tfidf, y_train, num_epochs=55)\n",
    "    _, accuracy = evaluate_model(nn_model, X_test_tfidf, y_test)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_hidden_layers_units = hidden_layers_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/ 100], Loss: 0.6929, Accuracy: 0.5368, Val_loss: 0.6932, Val_accuracy: 0.4932\n",
      "Epoch [10/ 100], Loss: 0.6923, Accuracy: 0.5325, Val_loss: 0.6930, Val_accuracy: 0.5372\n",
      "Epoch [15/ 100], Loss: 0.6917, Accuracy: 0.5773, Val_loss: 0.6929, Val_accuracy: 0.5236\n",
      "Epoch [20/ 100], Loss: 0.6906, Accuracy: 0.6323, Val_loss: 0.6926, Val_accuracy: 0.5203\n",
      "Epoch [25/ 100], Loss: 0.6894, Accuracy: 0.6492, Val_loss: 0.6920, Val_accuracy: 0.5541\n",
      "Epoch [30/ 100], Loss: 0.6879, Accuracy: 0.7109, Val_loss: 0.6916, Val_accuracy: 0.5608\n",
      "Epoch [35/ 100], Loss: 0.6861, Accuracy: 0.7726, Val_loss: 0.6909, Val_accuracy: 0.5777\n",
      "Epoch [40/ 100], Loss: 0.6838, Accuracy: 0.7658, Val_loss: 0.6897, Val_accuracy: 0.6216\n",
      "Epoch [45/ 100], Loss: 0.6811, Accuracy: 0.7777, Val_loss: 0.6889, Val_accuracy: 0.5878\n",
      "Epoch [50/ 100], Loss: 0.6781, Accuracy: 0.8546, Val_loss: 0.6875, Val_accuracy: 0.6149\n",
      "Epoch [55/ 100], Loss: 0.6740, Accuracy: 0.8732, Val_loss: 0.6857, Val_accuracy: 0.6318\n",
      "Epoch [60/ 100], Loss: 0.6692, Accuracy: 0.8850, Val_loss: 0.6831, Val_accuracy: 0.6959\n",
      "Epoch [65/ 100], Loss: 0.6627, Accuracy: 0.8994, Val_loss: 0.6803, Val_accuracy: 0.6993\n",
      "Epoch [70/ 100], Loss: 0.6545, Accuracy: 0.8977, Val_loss: 0.6763, Val_accuracy: 0.7534\n",
      "Epoch [75/ 100], Loss: 0.6434, Accuracy: 0.9172, Val_loss: 0.6714, Val_accuracy: 0.7432\n",
      "Epoch [80/ 100], Loss: 0.6287, Accuracy: 0.9256, Val_loss: 0.6656, Val_accuracy: 0.6791\n",
      "Epoch [85/ 100], Loss: 0.6076, Accuracy: 0.9256, Val_loss: 0.6549, Val_accuracy: 0.7568\n",
      "Epoch [90/ 100], Loss: 0.5805, Accuracy: 0.9349, Val_loss: 0.6435, Val_accuracy: 0.7264\n",
      "Epoch [95/ 100], Loss: 0.5415, Accuracy: 0.9476, Val_loss: 0.6243, Val_accuracy: 0.7736\n",
      "Epoch [100/ 100], Loss: 0.4889, Accuracy: 0.9552, Val_loss: 0.6028, Val_accuracy: 0.7466\n",
      "Elapsed time: 13.10s\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5855 - accuracy: 0.7551\n",
      "Loss:  0.5855062007904053 Accuracy:  0.7550607323646545\n",
      "Epoch [5/ 100], Loss: 0.6919, Accuracy: 0.5173, Val_loss: 0.6920, Val_accuracy: 0.4932\n",
      "Epoch [10/ 100], Loss: 0.6665, Accuracy: 0.7388, Val_loss: 0.6715, Val_accuracy: 0.6419\n",
      "Epoch [15/ 100], Loss: 0.5374, Accuracy: 0.7210, Val_loss: 0.5722, Val_accuracy: 0.6622\n",
      "Epoch [20/ 100], Loss: 0.4300, Accuracy: 0.7836, Val_loss: 1.3262, Val_accuracy: 0.4932\n",
      "Epoch [25/ 100], Loss: 0.0944, Accuracy: 0.9890, Val_loss: 0.4391, Val_accuracy: 0.8007\n",
      "Epoch [30/ 100], Loss: 0.0121, Accuracy: 1.0000, Val_loss: 0.4717, Val_accuracy: 0.8074\n",
      "Epoch [35/ 100], Loss: 0.0045, Accuracy: 1.0000, Val_loss: 0.4751, Val_accuracy: 0.8007\n",
      "Epoch [40/ 100], Loss: 0.0026, Accuracy: 1.0000, Val_loss: 0.5035, Val_accuracy: 0.8074\n",
      "Epoch [45/ 100], Loss: 0.0018, Accuracy: 1.0000, Val_loss: 0.5099, Val_accuracy: 0.8041\n",
      "Epoch [50/ 100], Loss: 0.0013, Accuracy: 1.0000, Val_loss: 0.5254, Val_accuracy: 0.8041\n",
      "Epoch [55/ 100], Loss: 0.0010, Accuracy: 1.0000, Val_loss: 0.5338, Val_accuracy: 0.8108\n",
      "Epoch [60/ 100], Loss: 0.0008, Accuracy: 1.0000, Val_loss: 0.5435, Val_accuracy: 0.8041\n",
      "Epoch [65/ 100], Loss: 0.0007, Accuracy: 1.0000, Val_loss: 0.5525, Val_accuracy: 0.8041\n",
      "Epoch [70/ 100], Loss: 0.0006, Accuracy: 1.0000, Val_loss: 0.5579, Val_accuracy: 0.8041\n",
      "Epoch [75/ 100], Loss: 0.0005, Accuracy: 1.0000, Val_loss: 0.5668, Val_accuracy: 0.8074\n",
      "Epoch [80/ 100], Loss: 0.0005, Accuracy: 1.0000, Val_loss: 0.5718, Val_accuracy: 0.8074\n",
      "Epoch [85/ 100], Loss: 0.0004, Accuracy: 1.0000, Val_loss: 0.5748, Val_accuracy: 0.8041\n",
      "Epoch [90/ 100], Loss: 0.0004, Accuracy: 1.0000, Val_loss: 0.5798, Val_accuracy: 0.8041\n",
      "Epoch [95/ 100], Loss: 0.0003, Accuracy: 1.0000, Val_loss: 0.5843, Val_accuracy: 0.8041\n",
      "Epoch [100/ 100], Loss: 0.0003, Accuracy: 1.0000, Val_loss: 0.5874, Val_accuracy: 0.8007\n",
      "Elapsed time: 14.34s\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6761 - accuracy: 0.7955\n",
      "Loss:  0.6761364936828613 Accuracy:  0.7955465316772461\n"
     ]
    }
   ],
   "source": [
    "# find best learning_rate\n",
    "best_accuracy = 0\n",
    "best_learning_rate = learning_rates[0]\n",
    "for learning_rate in learning_rates:\n",
    "    X_train_tfidf, X_test_tfidf, y_train, y_test, vocabulary_size = prepare_data(df, min_df=best_min_df, ngram_range=best_ngram_range)\n",
    "    nn_model = build_model(vocabulary_size, learning_rate=learning_rate, hidden_layers_units=best_hidden_layers_units)\n",
    "    nn_model, _ = train_model(nn_model, X_train_tfidf, y_train, num_epochs=100)\n",
    "    _, accuracy = evaluate_model(nn_model, X_test_tfidf, y_test)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_learning_rate = learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best accuracy: 0.7955465316772461\n",
      "best parameters: min_df=2, ngram_range=(1, 3), learning_rate=0.1, hidden_layers_units=[128, 64, 32]\n"
     ]
    }
   ],
   "source": [
    "print(f\"best accuracy: {best_accuracy}\")\n",
    "print(f\"best parameters: min_df={best_min_df}, ngram_range={best_ngram_range}, learning_rate={best_learning_rate}, hidden_layers_units={best_hidden_layers_units}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
